[toc]  

# 第一章 系统搭建过程步骤

## 一、markdown文件编辑方法
使用vscode编辑markdown文件，首先建立一个目录，在目录上使用右键菜单“用code”在左边的资源管理器中新建一个文件，例如：1.md。
![图 7](images/55047c1568e072cd611ab4b24082f169877659e7cac143cdc03ceeab98c16e92.png)  
安装三个扩展：
* Markdown All in One;
* Markdown Preview Enhanced;
* Markdown Image。
![图 0](images/0a012be1a4c7271770f3233b3229470fd2e93898ec771ac3832bbc208665bb6c.png)  
插图练习：剪切一个图片到剪贴板，在要粘贴图片的地方右键，选择粘贴图片
![图 1](images/d563c385bb545dfa640b5d1f44123b5773847d56f68308e628e967fb2d9ae6d5.png)

## 二、Linux
### 1 安装linux
使用VirtualBox安装ubuntu22.04,Desktop版本。
安装前，硬盘空白空间要大于20G。
#### 1.1 安装前配置
右键以管理员身份运行打开VirtualBox。点击新建：
![图 20](images/237ebe5053390430d47578d2b2a482fb2d1248307ab3aa8e322b3ecd24bf6e90.png)  
填入相关内容，点击下一步；
内存调整到8G，点击下一步；
![图 2](images/55a4b6fb912134e82cdf5ae90c0ec392a16fa02d4975c2beec039606a9a1b554.png)  
![图 18](images/c6c98c637fce4b9a9270552db43bf958ac285a6db280d81ab641b4553732f823.png)  
![图 19](images/4ec76a53d899df34f412a98732a3946868dc5120d3a80a862f902fb6eef8817f.png)  
虚拟盘大小建议大于60G，点击创建后设置完成。
检查设置：
* 系统：cpu核心数，可以选择物理核心数的一半；内存大于4G
* 观察网络形式，尽量不修改。
* 加载安装映像：
![图 16](images/6b1114be38b97a1147ee0b851d5f95c7e04bb9bc8d71bb02edde38943cbd4996.png)  
点击选择虚拟盘，只想下载后的iso文件。
#### 1.2 安装
点击“启动”，界面上选择“Install ubuntun”,到如下界面需要处理
![图 17](images/95e0278ef9bcf98f4c1020a961cc0e757515d2ec67a0243ebbb74bdd3eaf160f.png)  
去掉安装时更新选项
一直继续到选择时区，选择上海
![图 14](images/f21f878661d253f6189350a16852a5cb9fc4323d6b1370f15dda7de4c29bee67.png)  
![图 13](images/cf6f91e3dfe88c8bbb2106875c1dafa16ca9c8e0a5415c43b55a42c97c12a800.png)  
填入相关内容
用户名：bigdata3
密码：zufeeducn
![图 12](images/44d0c1e6a32c37e199c61d3aaea4ab2db060b0684e9da97e7ea10c0325195fb6.png)  
点击“重启”
![图 11](images/e996dbf20da01bd3d4dc5ab2f340dbc00c46879f8897e877b2cdbc899d87df4a.png)  
直接回车键重启，如果启动有问题，可以强制退出，然后退出安装映像，再启动
#### 1.3 安装后配置和常用软件安装
##### 1.3.1 安装增强功能（物理机没有）
增强功能解决问题：与host之间的显示匹配、文件共享、剪贴板功能共享。
从菜单设备->安装增强功能，界面多了一个光盘图标
![图 6](images/2227f656e676ecb0874924b63dd5ca521c2511a5c1ee8b6370d95b435ff796fd.png)  
双击打开光盘
![图 4](images/86fb3317fd1bf3c2a72e979e59f4fca4e4855e7f96db693eeee66e9fada79198.png)  
按惯例分析，应该是运行autorun.sh,在当前界面上点击右键，点击“Open in Terminal”  
![图 5](images/d971e59274ced373e764498a16e0b633f4bd7bd686f7c8f4681810a5e917ac23.png)  
输入后回车执行“./autorun.sh”
安装完成后重启，设置显示和粘贴板。
##### 1.3.2 安装软件
安装好linux后需要选择镜像源，为了下载速度快。设置方法：
点击左下角9个点，找到Software&Updates，将软件源改成国内的镜像。
打开终端快捷键：ctrl + alt + t
同步软件源：
```shell
sudo apt-get update
```
更新软件： 
```shell
sudo apt-get upgrade
```
两种方法：使用库安装，下载软件安装。
(1)库安装
安装openssh-server软件：sudo apt-get install openssh-server
安装vim编辑器：sudo apt-get install vim
(2)下载安装
如果库里没有这个软件，或者库里的版本不对，需要下载安装，以安装chrome为例，下载对应的chrome安装包，一般是.deb类型
安装有多种方式，建议用apt install安装，安装时要求全路径：
在软件所在目录下执行：sudo apt-get install ./google-chrome-stable_current_amd64.deb
##### 1.3.3 配置汉字输入法
设置->Region & Language->Manage Installed Language
显示需要安装时，允许安装，重启虚拟机。
设置->Keyboard->右侧界面+->选Chinese->选一个自己喜欢的输法，重启虚拟机。
在右上角可以看到汉语输入法的选项。
### 2 linux使用入门
#### 2.1 ls:列表显示当前目录下的目录和文件（常规属性）
不带参数的情况下，显示常规参数的文件和目录；
显示所有文件：带参数-a；
显示详细内容：带参数-l，或者直接用ll
#### 2.2 cd：切换目录
快捷操作：cd：回到家目录：根：/;当前目录：.;上一级目录：..;切换到历史路径：cd -，当前用户家目录：~。
#### 2.3 linux目录结构
linux的图形界面一般只能访问家目录内容。
![图 9](images/c7cd2fc6c61ad3ef772fd332d4d96cf5ef728d6f4f6775f1f78bf9ebc966108b.png)  
mount 挂载，指可切除设备（一般热插拔设备）连接到系统
#### 2.4 pwd:显示当前路径
#### 2.5 创建新文件：touch
#### 2.6 创建目录：mkdir
可以相当路径（从当前路径开始的），也可以绝对路径（从/开始的）。
![图 10](images/1c233aa75662e4c381c33db5dac73f3d0a2bdd76a44bb1645ad791f57ac76de9.png)  
#### 2.7 显示文件内容：cat,more,less
cat：显示所有文件内容
more:分页显示，但是只能前进
less：分页显示，可以前进后退
#### 2.8 创建新用户
用图形界面或者命令行（adduser或useradd）都可以。
可以创建新用户的前提是：当前用户拥有sudo权限
#### 2.9 切换用户
前提是：当前用户拥有sudo权限；或者另外一个用户的权限。
命令：su用户名
#### 2.10 vim的使用
vim有三种模式：命令、编辑、底行命令（或者说冒号命令）。
进入操作界面是命令模式，按i等进入编辑模式，按Esc键进入命令模式，在命令模式下按冒号键进入底行命令模式。
按esc后
保存退出  ：wq
直接退出  ：q！
课堂练习：新建一个用户（有sudo权限），切换到此用户，在此用户家目录下创建一个有100行数据的文件，用cat显示文件内容，再创建一个目录，将此文件移动到此目录下

## 三、hadoop软件安装及配置
### 1 spark、hadoop、java、scala版本选择
根据软件的层次结构，从spark开始选版本。打开https://spark.apache.org
![图 0](images/5ab4a4eb462e896b37b7c998d57bdc654b789420e398ea6963ada0807ff7b5ca.png)  
![图 1](images/88355845461982723c8e6a175854e6c966ce05909c1727b3b31c9d6a32b029ec.png)  
选择最新版spark3.5.0，hadoop选择3.3以上版本，Scala语言选择2.12。
从https://spark.apache.org查看需要的java版本
![图 2](images/d2be840a936e6369cd680ecce1c63561c050693d9b2194ef732027eda8603672.png)  
hadoop的最新稳定版本是3.3.6
![图 3](images/31c4baa6e67ba49e219c6509947df28ed8fb9dde993b8547dcddf43ea2af9bf1.png)  
![图 4](images/90df0ec0c60cafa185758d4a0403126256a318fc2a04f09f8cebd71b63082763.png)  
java应该选择java8，因为java11只能用于运行时，编译时不可以。
总结：
* spark选3.5.0
* hadoop选3.3.6
* Scala选2.12
* java选8
### 2 hadoop的local模式配置
### 3 下载hadoop软件
从官方网站或课程ftp下载
下载后解压缩。可以双击解压缩（有时候不行），命令行解压缩:
tar -zxvf hadoop-3.3.6.tar.gz
### 4 hadoop的基本概念
1）hadoop集群的三种运行模式：
Local(Standalone):本地模式(单机)
Pseudo-Distributed Mode: 伪集群(用java线程模拟)
Fully-Distributed Mode： 集群(多机)
2）hadoop包括的主要部分：
hdfs：分布式文件系统
mapreduce：计算框架
### 5 Local模式的安装和运行
#### 5.1 关闭集群内机器的安全设置
关闭防火墙，Selinux。ubuntu默认是关闭的。
检查防火墙状态：sudo ufw status
![图 10](images/a7d51bac66b5b72f49d5da93b893bca81796e004af8387ec1f155d3030efeed9.png)  
#### 5.2 软件准备
java和ssh。
##### 5.2.1 java
java采用用户安装模式：下载、解压缩、配置路径。
解压缩：tar -zxvf jdk-8u351-linux-x64.tar.gz
测试软件是否可用：全路径运行java，一般运行 java -version
![图 6](images/6dddd6b1c9886e1912e8d2cf380d1929ea22274a72a02e98fa9c17e4f4d44660.png)  
配置路径:配置的作用域：当前用户下。配置文件.bashrc
添加环境变量JAVA_HOME，指定java所在路径
```shell
#my
export JAVA_HOME=/home/bigdata3/software/jdk18
export PATH=$JAVA_HOME/bin:$PATH
```
![图 5](images/4b00cd1058dd552a22bcb1b4c99d52ab56634e0fe256db908f7915d0c0011865.png)  
测试：
![图 7](images/373b39d594a27a512602383ea240840f238b399c4edcc73d7f3489397c3376a9.png)  
##### 5.2.2 ssh
ssh和sshd
```shell
sudo apt-get install ssh
sudo apt-get install pdsh
```
#### 5.2 Local模式的启动
##### 5.2.1 配置
配置软件包下的文件etc/hadoop/hadoop-env.sh，添加java的根路径：
JAVA_HOME=/home/bigdata3/software/jdk18
![图 8](images/875a715926b1cb2d53388ec79c64a316042f6f22eca7bbce42fbd012a9161fb0.png)  
##### 5.2.2 测试启动hadoop
在没有配置系统路径的情况下，在软件包根目录下，执行：bin/hadoop
![图 9](images/9b14a24015b02cc36e09967dba65c4331570b7d4ddecdef37aa6a875cbdb448f.png)  
##### 5.2.3 配置hadoop路径
编辑 .bashrc,将hadoop路径写入PATH
```shell
export HADOOP_HOME=/home/bigdata3/software/hadoop-3.3.6
export PATH=$HADOOP_HOME:$PATH
```
![图 11](images/c40f12182f14872d3818d3b334e2e333ae915e064a1edb11fcdf1b71b8480d5e.png)  
测试：新开一个终端，执行hadoop
![图 12](images/f7fe36000c8e431c1cc9a8738161ff4178ecd5358eb9a300ae439122b528f1ec.png)  
##### 5.2.4 实例测试
在家目录下创建一个目录hadoopExample，进入此目录，新建一个目录input
拷贝输入文件：
```shell
cp $HADOOP_HOME/etc/hadoop/*.xml input
执行：
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar grep input output 'dfs[a-z.]+'
```
## 四、集群配置及使用
### 1 基本概念
1）namenode（master）
（1）在没有做高可用的情况下，一个集群有一个namenode，namenode存储系统的关键信息：
（2）hdfs的文件信息：文件属性、存储位置、监视节点状态，根据节点状态调整存储位置。master。
（3）mapreduce的任务调度和相关运行信息

2）datanode（worker或slave）
（1）hdfs的文件内容 
（2）计算的算力
### 2 配置文件
只读文件(默认配置)：core-default.xml,hdfs-default.xml,yarn-default.xml and mapred-default.xml
配置文件： etc/hadoop/core-site.xml,etc/hadoop/hdfs-site.xml,etc/hadoop/yarn-site.xml and etc/hadoop/mapred-site.xml
配置文件内容优先。
环境配置文件： etc/hadoop/hadoop-env.sh and etc/hadoop/yarn-env.sh
### 3 配置前节点准备
#### 3.1 关闭所有安全策略
一般是关闭防火墙和Selinux。
#### 3.2 域名文件准备
* 保证所有节点用户名相同,计算机（host）名不同
* 写域名解析文件hosts，包括所有节点
给每个节点起个不同的名字，例如：matser，slave0，slave1，slave2
改名的方式，修改/etc/hostname文件，文件内容就是计算机名。
要求所有节点使用固定ip。
#### 3.3 检查ssh软件安装
$ sudo apt-get install ssh
$ sudo apt-get install pdsh
#### 3.4 ssh免密设置
注意：一定要用域名
每个节点两步:(1)生成秘钥对；(2)传递公钥到所有节点(包括自己)。
##### 3.4.1 生成秘钥对
命令：
```shell
ssh-keygen -t rsa
```
全部选择默认。
传递前，确认秘钥没有重复传递，如果没有其他秘钥配置，可以直接删除.ssh文件夹。
##### 3.4.2 传递公钥到所有节点（包括自己）
```shell
ssh-copy-id 节点名
```
### 4 配置
#### 4.1 建立目录，拷贝文件，安装文本编辑器
![图 6](images/ac48f887fc67b5d9687331d1c5456a315f7134bb9e97830445c15f10624160dc.png)  
#### 4.2 配置hadoop-env.sh
```shell 
#my
export JAVA_HOME=/home/bigdata3/software/jdk18
export HADOOP_PID_DIR = /home/bigdata3/hadoopData/HADOOP_LOG_DIR
export HADOOP_LOG_DIR = /home/bigdata3/hadoopData/HADOOP_PID_DIR
```
![图 5](images/7a555829b9ee3b7c52dfa50a786e7f256edc38dfefdf0d0ade3c36b3b2c441e0.png)  
#### 4.3 配置core-site.xml
```shell
<configuration>
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://master:9000/</value>
</property>
<property>
  <name>io.file.buffer.size</name>
  <value>131072</value>
</property>
</configuration>
```
![图 7](images/be5b8b53a1a76b45c9f96ccf66810ac388a6dcc1daa8464eef6ecb5dd5655c94.png)  
#### 4.4 配置hdfs-site.xml
查看hdfs-default:dfs.namenode.name.dir和dfs.datanode.data.dir的默认配置
```shell
<property>
  <name>dfs.datanode.data.dir</name>
  <value>file://${hadoop.tmp.dir}/dfs/data</value>
  <description>Determines where on the local filesystem an DFS data node
  should store its blocks.  If this is a comma-delimited
  list of directories, then data will be stored in all named
  directories, typically on different devices. The directories should be tagged
  with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS
  storage policies. The default storage type will be DISK if the directory does
  not have a storage type tagged explicitly. Directories that do not exist will
  be created if local filesystem permission allows.
  </description>
</property>
<property>
  <name>dfs.namenode.name.dir</name>
  <value>file://${hadoop.tmp.dir}/dfs/name</value>
  <description>Determines where on the local filesystem the DFS name node
      should store the name table(fsimage).  If this is a comma-delimited list
      of directories then the name table is replicated in all of the
      directories, for redundancy. </description>
</property>
```
都指向了同一个变量：hadoop.tmp.dir,所以修改此变量即可，不用修改hdfs-site.xml的这两个配置了。通过查找，hadoop.tmp.dir在core-default.xml中有设置：
![图 8](images/2770ce639a0868f690814197f12fb8f93acf15e8147c27eb904d522275993922.png)  
所以修改core-site.xml的对应配置为：
```shell
<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/bigdata3/hadoopData</value>
  <description>A base for other temporary directories.</description>
</property>
```
![图 9](images/27009e86e1424812157a50f92aef118fce08535f026b83b5987f0b1442bfacae.png)  
dfs.blocksize默认是128MB，已经足够大了，不再进行配置了。
dfs.namenode.handler.count默认是10，对于我们做实验而言，已经足够了，不再进行配置了。
#### 4.5 yarn-site.xml
yarn.resourcemanager.address的默认配置：
![图 10](images/ff80bbec0e3dec0b822fc45ec2b1e913625df75f030ce0b0a41c31deaa24eafa.png)  
由于地址写的是0.0.0.0，所以启动此任务的机器就是resource manageer主机。
当前没有新配置。
```xml
<configuration>
  <property>
    <name>yarn.log.dir</name>
    <value>${hadoop.tmp.dir}/yarn-log-dir</value>
  </property>
<!--property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>${yarn.log.dir}/userlogs</value>
</property-->
  <property>
    <description>Where to aggregate logs to.</description>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>${hadoop.tmp.dir}/tmp/logs</value>
  </property>
</configuration>
```
![图 14](images/94734528dc9f5492332705b7db75559d13caa0b046b46d7b424ce570399ff8bd.png)  
#### 4.6 mapred-site.xml
默认配置:
![图 21](images/0e773cc23cd445f9e317ba77defcb31b2a8e33aaf34f39030c97af0c7b03700d.png)  
ip地址是0.0.0.0,意思是匹配当前ip地址。
mapreduce.framework.name默认是local,配置为yarn;
```xml
<configuration>
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
<property>
  <name>yarn.app.mapreduce.am.staging-dir</name>
  <value>${hadoop.tmp.dir}/tmp/hadoop-yarn/staging</value>
</property>
</configuration>
``` 
![图 15](images/b5945f6e52a16615d157ae14a1fae6e71d4f34b3ca52fdc13a772a1908dcbf54.png)  

### 5 集群启动及故障排除
#### 5.1 namenode格式化
namenode格式化只能进行一次，建议是在namenode节点机器上执行。格式化是在namenode节点上建立数据初始文件，集群启动时将进行所有节点的数据初始化。
如果进行第二次格式化，需要删除所有节点的数据文件夹。否则进入安全模式（Uber）。
格式化命令：
hdfs namenode -format 
![图 17](images/b55f9b8b4ce318638b4f9c6ce617d5247eadbf23e2a83cee4dd1c396c4fe3a80.png)  
#### 5.2 启动集群
可以用自动化脚本启动，如果在namenode节点上启动，在路径已经添加到环境变量PATH里的情况下（export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH），执行:start-all.sh
使用jps检测结果是：
namenode：
![图 26](images/82d6dbaf2f1f17851b97a82d60ef3c2a1aee0a43f256ffb1a679862648fa248a.png)  
datenode：
![图 25](images/009831c2bb25c75b0a4bd843c6c8c199d5e913f3982fae79b2f1ea8bf4c20764.png)  
使用web页面检测：
![图 24](images/57d2cd16460af6f0999332295eb3833d9ab1dc8f026adb473d03ff11ae9b7c78.png)  
![图 23](images/28c4bdc724339a7da26c176a9f13be11f7fde5a186954b8feb401a5d6debac41.png)  
### 6 hdfs集群的使用方式
#### 6.1 本地使用
hdfs client在集群内。
#### 6.2 远程使用
##### 6.2.1 不修改配置
hadoop -fs hdfs://namenode:port/ 操作命令
##### 6.2.2 修改配置
默认文件系统由core-site.xml中的fs.faulutFS定义的，指向域名master：9000
![图 4](images/5f84f1af105fd9d369b05cf19a19adf9a739c19c0f284631126c017c83b9eecb.png)  
修改方法1：
修改fs.defaultFS的value可以指向不同的远程集群
修改方法2：
不修改fs.defaultFS的value，修改系统域名解析文件中的master对应的ip地址。
### 7 hdfs命令
#### 7.1 -ls
列表路径下的文件和目录。
远程集群：
hadoop fs -ls hdfs://172.21.2.6:9000/路径
#### 7.2 -mkdir
创建新目录
本地集群（或者是配置了的远程集群）
hadoop fs -mkdir /目录名
远程集群：
hadoop fs -mkdir hdfs：//172.21.2.6：9000/学号（目录名）
#### 7.3 -put -copyFromLocal
从本地文件拷贝到hdfs
远程集群：
hadoop fs -put 文件名1 hdfs：//172.21.2.6:9000/路径/文件名2
文件名1与文件名2相同，文件名2可以不写。
#### 7.4 -get -copyToLocal
hdfs拷贝到本地
命令格式
远程集群：
hadoop fs -get 文件名2 hdfs：//172.21.2.6:9000/路径/文件名1
将hdfs上的文件1拷贝到本地，并改名为文件名2，如果不改名，文件名2可以不写
#### 7.5 -cat
显示文件内容
命令格式
远程集群：
hdfs fs -cat hdfs://172.21.2.6:9000/路径/文件名
#### 7.6 -appendToFile
#### 7.7 -cp
在hdfs上执行拷贝。
#### 7.8 -head
显示文件前部的内容.
#### 7.9 -moveFromLocal,  -moveToLocal, -mv
文件移动
#### 7.10 -rm
删除文件
hadoop fs -rm /012345678/master.txt
![图 9](images/87ca5d22747f5152a78bcdbfd7a1bb5ff65530883df91a2878643fbabbcae5f3.png)  
删除目录：
hadoop fs -rm /012345678/master.txt
![图 8](images/67d8583aaf593a4feee2f82ca5183cb0476913b6af89c95fbd4c1311116b37ee.png)  
#### 7.11 -rmdir
删除目录
hadoop fs -rm -r /012345678
#### 7.12 -touch
创建一个文件。
### 8 观察hdfs在本地的存储
在web页面点击要查看的文件：
![图 6](images/1bfb11d3cbc16e55491c8734f94a71546c5028cb84b2d28cd1d8de616aa7bca1.png)  
查看BLOCK_ID和存储节点位置，到相应的节点，根据BLOCK_ID就可以找到具体的文件
![图 5](images/fff763534b80dd26adaa7f7a248eecee45678312d1182e7eda07889f1c3686a3.png)  
![图 10](images/55d4ddbeba1a0a918209797fae4e7d36db1f319b221ce8ebc3f325b6def18c84.png)  

# 第二章 Scala
## 一、scala语言环境搭建
### 1 scala基本语言环境
下载scala2.12，解压缩：tar -zxvf scala-2.12.15.tgz
验证：进入根目录下的bin，执行：./scala
![图 1](images/533bedb9805c8829042574d50658243420744bcb7a00270532a6a249cbe9a198.png)  
配置路径，编辑.bashrc，添加环境变量SCALA_HOME，把scala命令的路径添加到PATH中。
```shell
export SCALA_HOME=/home/bigdata3/software/scala-2.12.15
export PATH=$SCALA_HOME/bin:$PATH
```
![图 2](images/19b72093450a34f934dae891e01964ff6818da8b58c22804ed4bdcc325f7b5ab.png)  
### 2 通过spark启动scala语言环境
下载spark3.5,解压缩：tar -zxvf spark-3.5.0-bin-hadoop3.tgz
验证：进入spark根目录下的bin，执行./spark-shell
![图 3](images/faa541a9f80bf9425d0a3a6e9c6e27d25690147631053e460090c0859c8a4318.png)  
配置路径，编辑.bashrc，添加环境变量SPARK_HOME，把spark命令的路径添加到PATH中。
```shell
export SPARK_HOME=/home/bigdata3/software/spark-3.5.0
export PATH=$PARK_HOME/bin:$PATH
```
![图 4](images/2a3f897998307be8b3a3a8aa79b21f270d2336fe6e2582e76b6f98d559f999db.png)  
验证：
![图 5](images/ed2ca0483935f8e33397890eadaaa5b982b355a7cabba8f0e1434cdadadeeb04.png)  
### 3 scala集成开发环境
下载，解压缩 tar -zxvf ideaIC-2023.2.4.tar.gz
将路径添加到PATH：
```shell
export PATH=/home/bigdata3/software/idea-IC-232.10203.10/bin:$PATH
```
![图 10](images/64d75b4fcd063632a5e4a20016adac9d8267e7fe6477d5a2bbaa2c25c8756329.png)  
启动idea：在终端窗口，输入idea.sh
![图 12](images/4083855f34125dc51ef49756c136a30c9637e2ee3b4b84f244e273bb6c6ab021.png)  
选择NEW Project：
![图 9](images/cf590fd53929ec322d394a402ebd7d0329946862f281a7d59537911d98df1a4b.png)  
点击“OK”，重启IDE。
重启后，再选择New Project,输入Project名字，选择scala语言:
![图 11](images/c166755d45e580039482fa1e27ba998460b9cee08b021360768b4cb6b7d36c92.png) 
Build system选择IntelliJ,Scala SDK选择create：
由于我们在.bashrc中配置了SCALA_HOME,所以可以自动找到。
![图 13](images/ca997ce1d2f2cef7ca26923ae526245ab806f64f33c2273cc6d1a7726333370d.png)  
会自动生成一个hello world！程序，运行此程序。
![图 14](images/831676b7ff55a5f560ea609600136f69e76987f580a9b1df4290efa7bfa780d1.png)  
## 二、scala语言
### 1 变量声明
####  1.1 var和val
必须要使用var(可变)或val。
数据类型可以不写，系统会自动判断，但是注意是否符合要求。
自动判断规则：
* 整数：Int
* 小数：Double
#### 1.2 一定要初始化
#### 1.3 指明数据类型
* 如果要指明数据类型，需要在变量名后加“:”
* 初始化时使用数字后的标识
### 2 基本数据类型
String类型使用的是java的。
### 3 算术运算符
与其他语言类同，与java相同。
### 4 关系运算符
与其他语言类同，与java相同。
### 5 逻辑运算符
与其他语言类同，与java相同。
### 6 位运算符
二进制对应位的逻辑运算，结果是个数字。
位运算符有三个:与&，或|，异或^
![图 7](images/bb1f95b151d793a378b700c8745b1d8798625fa371cf8d3f217d475447a64490.png)  
### 7 赋值运算符
注意复合赋值运算符
### 8 返回值
scala语言任何一个语句块都可以有返回值，语句块的边界可以是逻辑边界，也可以明确用{}表示。
![图 8](images/d0d44438b64462c2ca46942798cff89faeea672929f9116c55eacccb8e3c6530.png)  
### 9 条件语句
#### 9.1 简单条件语句写法：写在一行
![图 16](images/b8d8a707e569e9daea960cf1baa73cb867d130546df50466385d33c8abb34a38.png)  
if最常用的场合是守卫。
#### 9.2 写在不同行
守卫用法：在其他结构里嵌入if语句的应用方式。
### 10 for
#### 10.1 for的写法
语法：
for(变量-<集合){
    循环体    
}
Range使用to或until,to包括头尾，until不包括头和尾，没有step。
for加守卫的写法：打印出1-10的偶数：
```java
object Main {
  def main(args: Array[String]): Unit = {
    for(i<- 1 to 10  if(i%2==0) ) {

        println(i)
    }
  }
}
```
for语句的守卫可以并行写，各个if之间默认是与的关系，例如：
```java
object Main {
  def main(args: Array[String]): Unit = {
    for(i<- 1 to 10  if(i%2==0) if(i!=4)) {

        println(i)
    }
  }
}
```

```java
object Main {
  def main(args: Array[String]): Unit = {
    for(i<- 1 to 10  if(i%2==0&&i!=4)) {

        println(i)
    }
  }
}
```
#### 10.2 嵌套for
scala语言的风格是先写循环部分，再写守卫。
写法：
```java
object Main {
  def main(args: Array[String]): Unit = {
    for(i<- 1 to 10;j <- 11 to 13;k<-14 to 16) {
        println(i*j+k)
    }
  }
```
例1：计算i*j,i从1到10，包括端点,但只要奇数,j从21到23，不包括23.
```java
object Main {
  def main(args: Array[String]): Unit = {
    for(i<- 1 to 10 if(i%2!=0);j <- 21 until 23 ) {
        println(i*j)
    }
  }
}
```
例2：打印出1~100的素数

### 11 while
与其他语言相同。
scala流程控制中没有break和continue。如果一定要用，需要手工导包。
### 12 do while
与其他语言相同。
### 13数据结构
#### 13.1 Array
注意写法，用圆括号。
默认是不可变数组（与大部分语言基本相同）。
如果要使用可变数组，需要手工导包。
#### 13.2 Map
键值对（与Python中的字典类似），写法不同，scala的写法：
val a=Map("a"->1,"b"->2)
#### 13.3 Tuple
与其他语言相同。
访问方式，例如：a._2,表示访问元组a中的顺序为2的数据，注意：顺序是1开始。
#### 13.4 Set
与其他语言相同。
### 14 scala函数
#### 14.1 普通函数
定义：
* 方法1，完整定义：
def 函数名（参数列表）：返回类型={函数体}
参数列表写法：参数名：数据类型
函数体中最后一个执行的语句的值就是返回值，一般不用return。
调用方式，与其他语言相同。
参数的类型不能省略，返回值的类型可以省略。
* 方法2.匿名定义：
（参数类型列表）=>{函数体}
#### 14.2 高阶函数
参数或返回值是函数的函数。
##### 14.2.1 参数是函数
###### （1）定义
参数是函数，就可以通过传入不同的函数，进行相应的处理。
定义：与普通函数区别不大，区别就是：作为参数的函数的类型写法。
def(参数列表)：返回值={函数体}
区别在于形参列表：作为参数的函数类型写法：(参数类型列表)=>返回值类型
调用函数时，函数形参位置代入已定义的函数名或匿名函数。
例如：
```java
object Main {
  def main(args: Array[String]): Unit = {
    def add1(a1:Int=10,a2:Int=20):Int={a1+a2}
    def mul1(a1:Int=10,a2:Int=20):Int={a1*a2}
    def cal(f1:(Int,Int)=>Int,a1:Int=10,a2:Int=20):Int={f1(a1,a2)}
    // val b=(a1:Int,a2:Int)=>{a1+a2}
    print("add(1+2)=");println(cal(add1,2,1))
    print("mul(3*2)=");println(cal(mul1,2,3))
  }
}
```
###### （2）默认参数问题
与其他语言一样，如果函数形参中有默认值，需要写在形参表的后面，包括有默认值的函数形参。
调用时，有默认值的形参可以不传实参
```java
object Main {
  def main(args: Array[String]): Unit = {
    def add1(a1:Int=10,a2:Int=20):Int={a1+a2}
    def mul1(a1:Int=10,a2:Int=20):Int={a1*a2}
    def cal(f1:(Int,Int)=>Int,a1:Int=10,a2:Int=20):Int={f1(a1,a2)}
    print("add(1+2)=");println(cal(add1,2,1))
    print("add(Null+Null)=");println(cal(add1))
    print("mul(3*2)=");println(cal(mul1,2,3))
    print("add(Null)=");println(cal1(mul1))
    print("add(add1,a2=30)=");println(cal1(a2=30))
  }
}
```
匿名函数作为函数的参数：
当作为函数的参数的函数比较简单，而且只使用一次的情况下，一般使用匿名函数作为函数的实参。
例如：
```java
object Main {
  def main(args: Array[String]): Unit = {
    def cal(f1:(Int,Int)=>Int,a1:Int=10,a2:Int=20)={f1(a1,a2)}
    print("add(1+2)=");println(cal((a:Int,b:Int)=>a+b,1,2))
  }
}
```
练习
def cal(f1:(Int,Int)=>Int,a1:Int=10,a2:Int=20)={f1(a1,a2)}，调用cal，计算（100+200）/(100+200)
```java
object Main {
  def main(args: Array[String]): Unit = {
    def cal(f1:(Int,Int)=>Int,a1:Int=10,a2:Int=20)={f1(a1,a2)}
    println(cal((a:Int,b:Int)=>(a*b)/(a+b),100,200))
  }
}

```
###### （3）SCALA的标识符（重点是_）
标识符与java和c基本相同：包含字母、数字、下划线，一般也主张使用驼峰命名法。重点是_的使用。
在高阶函数中，可以合理地使用_作为通配符，简化匿名函数的写法：在调用位置，当形参使用通配符时不会引起混淆，同时在函数体中此形参只使用一次的情况下，可以不写参数表，直接写函数体。
例如：
```java
    def cal(f1: (Int, Int) => Int, a1: Int = 10, a2: Int = 20) = {
      f1(a1, a2)
    }
    print("add(1+2)=");
    // println(cal((a: Int, b: Int) => a + b, 1, 2))
    println(cal((_+_),1,2))
```
![图 18](images/90f2fada38578dcab8781a084a17b84b69eb1f839c82a862c421d4e42ee0f967.png)  

##### 14.2.2 函数的返回值是函数
例如：
```java
object Main {
  def main(args: Array[String]): Unit = {
    def cal(a1:Int=10):Unit{
    if(a1<0) (a1:Int,a2:Int) => a1+a2
    else (a1:Int,a2:Int) => a1*a2
    }

    val ab = cal(a1=-3)
    val cd = ab(2,3)
    print(cd)
    print(cal(-3)(2,3))
  }
}
```
作为返回值的函数是此函数中定义的，如上例所示。否则需要在返回的函数后加下划线进行指明范围在语句块外。
例2：返回函数的定义在函数中，但在语句块外
```java
object Main {
  def main(args: Array[String]): Unit = {
    def cal(a1: Int = 10)= {
      def add1(a: Int, b: Int) = a + b
      def mul1(a: Int, b: Int) = a + b
      if (a1 < 0) add1 _
      else mul1 _
    }

    val ab = cal(-3)
    val cd = ab(2, 3)
    println(cd)
    println(cal(-3)(2, 3))
  }
}
```
例3：返回函数的定义在当前函数外：
```java
object Main {
  def main(args: Array[String]): Unit = {
    def add1(a: Int, b: Int) = a + b
    def mul1(a: Int, b: Int) = a + b
    def cal(a1: Int = 10)= {
      if (a1 < 0) add1 _
      else mul1 _
    }
    val ab = cal(-3)
    val cd = ab(2, 3)
    println(cd)
    println(cal(-3)(2, 3))
  }
}
```

# 第三章 Spark
## 一、绪论
### 1 数据集RDD
RDD：弹性分布式数据集
### 2 DAG
spark的计算过程是DAG(有向无环图)形式。
计算分为转换计算和执行计算。转换计算的结果还是RDD，执行计算的结果一般不是RDD，所以一般最后一步是执行计算。
### 3 Executor
在worker节点上的，负责运行task。
### 4 Task
工作单元
### 5 Job
### 6 Stage
## 二、RDD的输入输出从
### 1 生成RDD(输入)
jvm一般有一个context，spark的context，在spark-shell下，自动创建：sc。
![图 1](images/4f74915c774bf50b4c32abae21f9821f8f15e789faa2ca2349d37e620aff339b.png)  
![图 2](images/daf8333ba07141fe8dc607608c32bbcf043fcfa78329cbc4cf777aa622b14605.png)  
spark context命名为：sc，模式是local，分区数[*]:意思是由系统决定分几个区，如果要确定分区，需要把*改成想要的分区数，但在spark-shell下默认是[*]。
#### 1.1 从文件生成RDD
例如：从文本文件生成RDD：sc.textFile
![图 3](images/810e270e4647e40de82131b615effa2b80a025ab8949fed41a882d6408c7d764.png)  
textFile是惰性操作，需要action操作才执行，否则只检查语法、记录操作过程。
#### 1.2 从程序中的数据生成RDD
例如：parallelize
![图 13](images/76f6ca26786389f56c6c8447109a575376c07734f7cc46235a7a6feec549a2f8.png)  
### 2 RDD的输出
#### 2.1 print相关的
先用动作操作生成相应的数据结构，然后使用print。local模式下，可以直接看到结果：cluster模式下，在log文件里查看结果。
例如：
![图 11](images/5e991628b4310af64fbaf2563c04fe9e7ddd047e3e0e04f33c077de4770db4ed.png)  
#### 2.2 写入文件
例如文本文件，saveAsTextFile(url)。此处的url是路径(目录)，而且要求是不存在的目录
![图 4](images/9e000f14d5c85357a312728ed9d43af6c2416a33768369c2cb0436d11fa5047f.png)  
![图 5](images/cf2430bc9ca616e09db1876227f02e481bc538f0da2ee48a488501f8a9c0234f.png) 
saveAsTextFile保存的结果中有三个文件，其中_SUCCESS用文件名说明保存状态，其他两个文件是保存内容，说明当前RDD有2个分区。
## 三、常用转换操作
![图 6](images/2b7e2fc7fe994653869825dc60e5541b4b034c6df0172921aa36c52505c75212.png)  
![图 7](images/5fd0a59b682231d880cecd32d64f37cdaf04174caec0c911b3a8fef0f67845ad.png)  
![图 8](images/4524192e63f13f529b7fc83949d79080662653b62595abe9a36ba66a5eef3a8f.png)  
### 1 filter
使用方法：rdd.filter(func),其中func的返回值要求是布尔值:真：留下。
![图 1](images/9af0a60784594c7291d4acd62d336288980676379b72d318d1ddfbb61adc8c85.png) 
例如：将包含good的数据项留下，其他的抛弃：
![图 2](images/e6d5c31c4672fb3ce97f65d9760902e0b4d4bfe455941aa683d426e788dcf9a6.png)  
练习：不包含good的留下，并用_简写
例二：留下字符串长度>5的数据项
### 2 map
使用方法：rdd.map(func),将rdd中的每个元素按照func进行处理，生成一个新的RDD。
例如：将每个元素+1：
![图 3](images/efd3e9901cce21d7fc91af53c5153aec52198edcaeb6e8703039ff7c6bc3d193.png)  
### 3 flatmap
使用方法：rdd.flatMap(func),将rdd中的元素界限去除，执行func。
对一个英文文档，按空格进行分词：
![图 4](images/23398c2350007001282a396b007b162f5cf8e57759db55e9a0a29f45a8ff3732.png)
### 4 reduceByKey
用法：rdd.reduceByKey(func):应用于(K,Y)键值对的数据集时，返回一个新的(K，Y)形式的数据集，其中每个值是将每个key传递到函数func
例1：将上例中分出来的词组成(word,1)形式，可以看出K-V对中进行聚合后的结果
![图 5](images/24f413f1a7bd444b4ede8f1049ec8486cf8d0a0de54f4969ba10d81b0c47c166.png)  
例2:将上例生成的(k,v)形式的数据进行加法运算。
![图 6](images/667a4ee0fea5a5c64315ed76223feb65ed37ec64d910939bd57bc9b91a72eb37.png)
### 5 分区控制
#### 5.1 环境搭建时分区数指定
local模式下，可以直接在.setMaster('local[*]')中指定，其中*用分区数替换。
集群模式下，一般由集群控制。
#### 5.2 创建RDD时
创建RDD时一般有一个参数，用于指定最少分区数量。
例如：指定最少分区数4
```java
import org.apache.spark.{SparkConf,SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc=new SparkContext(conf)
    val Rdd1=sc.textFile("/home/bigdata3/word.txt")
    val rdd1=sc.textFile("/home/bigdata3/word.txt",4)
      .flatMap(_.split(""))
      .map((_,1))
      .reduceByKey(_+_)
    rdd1.foreach(println)
    Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
运行状态：
![图 8](images/2dfbd8ff0280dbd4b2ef6234905a2b690da0ef130774a4f185f968b8b2d38359.png)  
#### 5.3 有些转换操作(一般是宽依赖)有分区参数，可以进行重新调整分区数
本wordCount程序中只有一个宽依赖方法：reduceByKey,有第二个参数，可以指定分区。
4个分区的saveAsTextFile：
![图 9](images/e4d8e4cc5c5f71577016e26085dedc74c22210f80bbbf8146587645fb4f3a50b.png)  
通过reduceByKey调整为1个分区，然后saveAsTextFile：
```java
import org.apache.spark.{SparkConf,SparkContext}

  object Main {
      def main(args: Array[String]): Unit = {
        val conf = new SparkConf().setAppName("firstSpark").setMaster("local[5]")
        val sc = new SparkContext(conf)
        val rdd1 = sc.textFile("/home/bigdata3/word.txt", 4)
          .flatMap(_.split(" "))
          .repartition(4)
          .map((_, 1))
          .reduceByKey(_ + _, 1);
        rdd1.saveAsTextFile("/home/bigdata3/sparkOut/10")
        Thread.sleep(1000 * 60 * 5)
        sc.stop()
      }
  } 
```
运行情况:
![图 10](images/3d91b6e1b9fb85828777c3e9be350d8e8a62ebeb6ad01bd08cf6d67f99fafe0b.png)  
stage的分区数变成了1.
saveAsTextFile结果：
![图 11](images/3ecb25806f03a416e8c98374a0f6459dd4d203e067515328e35ad5a86b52136c.png)  
#### 5.4 用重新分区操作
#### 5.5 查看分区策略
用程序查看创建RDD时的分区情况：
```java
import org.apache.spark.{SparkConf,SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc=new SparkContext(conf)
    val rdd1=sc.textFile("/home/bigdata3/wordTest",4)
    //  .flatMap(_.split(" "))
    //  .map((_,1))
    //  .reduceByKey(_+_,1);
    rdd1.saveAsTextFile("/home/bigdata3/sparkOut/14")
    // Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
分区情况：
![图 12](images/f822ff7b59188af2476f8a89dd85f6a93451640ccd21c8a66ae0428b6b30518e.png)  
生成RDD时分区中的数据非常不平衡，将造成后续的计算效率低，需要在合适的位置调整分区。
根据本程序，在分词后重新分区比较合适：
```java
import org.apache.spark.{SparkConf,SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc=new SparkContext(conf)
    val rdd1=sc.textFile("/home/bigdata3/wordTest",4)
      .flatMap(_.split(" "))
      .repartition(4)
    //  .map((_,1))
    //  .reduceByKey(_+_,1);
    rdd1.saveAsTextFile("/home/bigdata3/sparkOut/18")
    // Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
分区情况：
![图 13](images/9f3dc2485b2cefc964c0b7d5a246c4ffb74eca2bbe5d1486091d4e726ca3e331.png)  
从结果可以看出，数据非常平衡。
#### 5.6 加入数据平衡环境后的wordCount
```java
import org.apache.spark.{SparkConf,SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc=new SparkContext(conf)
    val rdd1=sc.textFile("/home/bigdata3/wordTest",4) //args(0)) )
      .flatMap(_.split(" "))
      .repartition(4)
      .map((_,1))
      .reduceByKey(_+_,1);
    rdd1.saveAsTextFile("/home/bigdata3/sparkOut/17")
    Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
运行情况：
![图 14](images/da4af02fe568a0bf7b00eabdfcaaf7de6205720178ff84e9cbc0eda1a22f6c54.png)  
stage有三个：
因为repartition、reduceByKey有shuffle操作，即跨区数据交换。
分区数:
stage0生成RDD时是5个分区:textFile("home/bigdata3/wordTest",4)的第二个参数意义是最少分区数，也就是分区数>=4,考虑环境setMaster("local[5]")。
stage1分区数是4：repartition（4）
stage2分区数是1：reduceByKey（_+_,1）
## 四、常用动作操作
![图 9](images/fc97c2d461a61ddbf8ed80303844f5dc9890861aa036efc08e8ec48a7f9e4878.png)  
![图 10](images/606c8643893d873b2a8bd35154ab9c81cd2b6089301a8b5125f8c7f2dc5b8555.png)  
### 1 collect
将RDD转换为数组。
### 2 take(n)
取出前n个元素。
### 3 first
取出第一个元素。
### 4 foreach(func)
对每一个元素执行func。

## 五、Spark集成开发环境建立
打开Idea，从菜单New->Project：
![图 17](images/0b425ca6839988353e3654a8c8f01b1daf1963d9e55fbdaa2c96b989af47cbef.png)   
添加spark库文件：
菜单File->Project Structure
![图 18](images/f1f65adf81d5df5f652a2f66e6b0687a775622f40813af7ac88eda207a5bf0cd.png)  
点击左侧Global Libraries，再点击中间栏的+号，选java
![图 19](images/485790cf4ded769ff0776e0524f6a3e3e86bf5cb90cc23b93c03410e90d467a7.png)  
找到spark软件下的jars目录，选中所有，点击ok。等右下角indexing过程完成，才可以进行下一步。
完成程序
```java
import org.apache.spark.{SparkConf,SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc=new SparkContext(conf)
    val Rdd1=sc.textFile("/home/bigdata3/b.txt")
    Rdd1.saveAsTextFile("/home/bigdata3/sparkOut/5")
    Thread.sleep(1000*60*5)
  }
}
```
![图 20](images/e65e5e513163b189c981f56ce4db1c702daae35e1256ec09ce8854a603a8b68b.png)  
DAG图：
![图 21](images/093f22373c21cd41f7b6f65176b3b3e0bde5e0d7180e2a04b0d90fb633937469.png)  

## 六、用链式写法写出wordCount程序
### 1 spark-shell下：
![图 11](images/11e4ec8af68ee151d1b787206fd92127f1e59392f9bb1da6564ae47ceb58168b.png)  
```java
sc.textFile("/home/bigdata3/word.txt").flatMap(x=>x.split(" ")).map((_,1)).reduceByKey(_+_).collect
```
### 2 集成开发环境下
#### 2.1 程序运行时不使用参数
```java
import org.apache.spark.{SparkConf, SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc = new SparkContext(conf)
    sc.textFile("/home/bigdata3/word.txt")
      .flatMap(_.split(" "))
      .map((_,1))
      .reduceByKey(_+_)
      .foreach(println)
    Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
DAG:
![图 16](images/6fc63d535e64d70151116adfb9132112c7fa6d7e0c595c460c8d6cb0132058c1.png)  
从图种可以看出：一共有两个stage，因为只有reduceByKey是宽依赖，有shuffle操作(即分区之间的数据交换)，stage以shuffle操作划分：flatMap、map都是窄依赖。
job一般按照行动操作数量。
例如：
```java
import org.apache.spark.{SparkConf, SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc = new SparkContext(conf)
    val rdd1=sc.textFile("/home/bigdata3/word.txt")
      .flatMap(_.split(" "))
      .map((_,1))
      .reduceByKey(_+_)
    rdd1.foreach(println)
    rdd1.saveAsTextFile("/home/bigdata3/sparkOut/5")
    Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
有2个job：
![图 17](images/98b807bc83a5851080ba706f3679c06560d7e8b5c95698c4980c8c05344c4bea.png)  
其中job0的DAG：
![图 18](images/d3fbde9b570c1aded81e826e9f44349edc90ad45efcc2134d865345b25066fea.png)  
job1的DAG：
![图 19](images/9439aa383e373d1c31a3145deb7c2b399ed99b8e27dbfdb239cea4768fe45200.png)  

## 七、程序运行参数的使用
```java
import org.apache.spark.{SparkConf,SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("firstSpark").setMaster("local[3]")
    val sc=new SparkContext(conf)
    val rdd1=sc.textFile(args(0))//"/home/bigdata3/wordTest",4)
      .flatMap(_.split(" "))
      .repartition(4)
      .map((_,1))
      .reduceByKey(_+_,1);
    rdd1.saveAsTextFile("/home/bigdata3/sparkOut/16")
    Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
### 1 在集成开发环境中，使用运行时参数
菜单->Run->Edit Configuration:
![图 0](images/56a57c0294fd99d33cdf9a378821ebf72389c61d28b1ba71a1aac42500e9b9ed.png)  
点击add new configuration，选application  
![图 2](images/e2c329b00c97a7429c6a6fd1ded23bc936ff7be172b9500856090fa23a5ce090.png)  
### 2 在spark-submit时使用参数
## 八、程序打包
### 1 修改程序
修改程序，一般删除.setMaster("local[5]")，在运行时指定运行方式。
```java
import org.apache.spark.{SparkConf,SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("firstSpark")//.setMaster("local[5]")
    val sc=new SparkContext(conf)
    val rdd1=sc.textFile(args(0))//"/home/bigdata3/wordTest",4)  )
      .flatMap(_.split(" "))
      .repartition(4)
      .map((_,1))
      .reduceByKey(_+_,1);
    rdd1.saveAsTextFile(args(1)) //"/home/bigdata3/sparkOut/20")
    //Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
### 2 打包前配置
菜单->File->Project Structure->Artifacts
![图 3](images/c49942e681e0665e13c5e3ad1dc4697de307fec410bf4e013eaddbd89121f00a.png)  
点击中间栏的+，
![图 4](images/5017c17554ce27759fbc1669e1c3dacfdd7bdecd96f4ede986237192070d5cff.png)  
选择Class,观察输出路径。点击Ok：
![图 5](images/4bbd200a4d9a85095a3f5f813e4450a8e9f7f2fbc1cfa69d4e320d1839f06703.png)  
减小jar包的占用空间：删除没用的库文件
![图 6](images/9eef1623288426f15302f6494d766a9f3d53e12dfcb7f5214c244b4b3644a600.png)  
点击ok，完成打包配置。
### 3 打包
菜单->Build->Build Artifacts:
![图 7](images/bf7135150eb123ed7aef320956be79abb5665c8ad2fee26d5c7195492ac35d9f.png)  
如果第一次build，选择Build；如果不是第一次，选择Rebuild。
观察生成的jar包：
![图 8](images/e8a1e8a0bb837f18830eaba29b94f2a307aaec6a242f9d9fa2abf24769fded1b.png)  
## 九、spark配置
配置前提：spark生成的jar打算运行在hadoop集群上，只需要配置spark-submit所在的host即可。配置文件的位置在spark软件的conf下。主要配置文件是spark-env.sh。
需要配置的是：
![图 9](images/22d9c2f870b242daf2b1fa7acdf3192e3e1d93a399f0ff3239e5f5190c2a7864.png)  
## 十、运行jar包
### 1 local模式运行
用spark-submit运行。
因为spark-submit命令太长，所以写一个运行脚本。
新建一个文件，写如下内容（local模式运行）：
```shell
spark-submit --master local[5] --deploy-mode client \

        /home/bigdata3/IdeaProjects/firstSparkProject/out/artifacts/firstSparkProject_jar/firstSparkProject.jar \

       "file:///home/bigdata3/wordTest" "file:///home/bigdata3/sparkOut/30"   
```
注意：
* 因为我们现在制定了hadoop的配置是集群的配置，默认文件系统是hdfs文件系统，所以如果是本地文件的话，必须是完整的URL，例如：file：//路径。
* hosts文件中必须有master的域名解析。
运行脚本文件：
bash mySparkSubmit.sh
### 2 集群模式运行
运行脚本：
```shell
spark-submit --master yarn --deploy-mode cluster \

        /home/bigdata3/IdeaProjects/firstSparkProject/out/artifacts/firstSparkProject_jar/firstSparkProject.jar \

       "/wordTest" "/sparkOut/31"   
```

# 第四章 电影评分案例
## 一、数据集
ratings.dat格式：
UserID::MovieID::Rating::Timestamp
users.dat格式：
UserID::Gender::Age::Occupation::Zip-code
movies.dat格式：
MovieID::Title::Genres
## 二、找出评分最高的10部电影名
### 2 思路
#### 2.1 评分最高的依据
Rating的平均值是最合理的。
计算平均值需要3步：求rating和、求rating次数、、做除法。
#### 2.2 求平均值
##### 2.2.1 求rating的和
步骤：数据生成RDD、数据分割、取出有用数据、计算rating和。
(movieID,Rating)
Array(UserID,MovieID,Rating,Timestamp)
```java
package zufe.info.bigdata.xhm431.movieRating

import org.apache.spark.{SparkConf, SparkContext}

object movieRating {
  def main(args: Array[String]): Unit = {

    val conf=new SparkConf().setAppName("movieRating").setMaster("local[*]")
    val sc=new SparkContext(conf)
    val ratings=sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
      .map(_.split("::"))
      .map(x=>(x(1),x(2)))
      .map(x=>(x._1,x._2.toDouble))
      .reduceByKey(_+_)
    ratings.take(30).foreach(println)

  }
}
```
##### 2.2.2 求ratings的个数
```java
    val ratingsCount=ratings
      .map(x => (x._1, 1.toDouble))
      .reduceByKey(_ + _)
```
##### 2.2.3 求平均值
必须是同一个的movied
```java
    val sumRatingsCounts=sumRatings.join(ratingsCount)
    val avgRatings=sumRatingsCounts.map(x=>(x._1,x._2._1/x._2._2))
```
#### 2.3 加入电影名并排序
```java
    val movies =sc.textFile("/home/bigdata3/ml-1m/movies.dat")
      .map(_.split("::"))
      .map(x => (x(0), x(1)))
      .join(avgRatings)
      .map(x=>(x._2._2,(x._1,x._2._1)))
      .sortByKey(false)
```
#### 2.4 优化
##### 2.4.1 优化前的程序及DAG
优化前的程序：
```java
package zufe.info.bigdata.xhm431.movieRating

import org.apache.spark.{SparkConf, SparkContext}

object movieRating {
  def main(args: Array[String]): Unit = {

    val conf=new SparkConf().setAppName("movieRating").setMaster("local[*]")
    val sc=new SparkContext(conf)
    val ratings=sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
      .map(_.split("::"))
      .map(x=>(x(1),x(2)))
    val sumRatings=ratings
      .map(x=>(x._1,x._2.toDouble))
      .reduceByKey(_+_)
    val ratingsCount=ratings
      .map(x => (x._1, 1.toDouble))
      .reduceByKey(_ + _)
    val sumRatingsCounts=sumRatings.join(ratingsCount)
    val avgRatings=sumRatingsCounts.map(x=>(x._1,x._2._1/x._2._2))

    val movies =sc.textFile("/home/bigdata3/ml-1m/movies.dat")
      .map(_.split("::"))
      .map(x => (x(0), x(1)))
      .join(avgRatings)
      .map(x=>(x._2._2,(x._1,x._2._1)))
      .sortByKey(false)
    movies.take(10).foreach(println)

  }
}
```
![图 17](images/843862c0fb95c5e0253b37bd9b19f58ea90c5e629487a49322a442d2d93319ea.png)  

![图 18](images/5f150beed2a1352a17df95a7d13fc194ec49bc0a90ce7db98daee4e87ea7595d.png)  

![图 19](images/0d5351daa0a3283a155d413171c9c4650910a9f858a3c69ca060237c121747cd.png)  

根据DAG：
![图 20](images/d79e78d6e92e7ef31041cd50cf2e0eed9baa4cae0246d2afb0963bc30506011e.png)  
问题在：输入数据相同，分成了2个分支，又合成了1个分支，执行效率会很差，同时程序结构也非常差。优化方法：本部分从开始到结束使用一个分支。
##### 2.4.2 优化后的程序及DAG
```java
package zufe.info.bigdata.xhm431.movieRating

import org.apache.spark.{SparkConf, SparkContext}

object movieRating {
  def main(args: Array[String]): Unit = {

    val conf=new SparkConf().setAppName("movieRating").setMaster("local[*]")
    val sc=new SparkContext(conf)
    val avgRatings=sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
      .map(_.split("::"))
      .map(x=>(x(1),(x(2).toDouble,1.toDouble)))
      .reduceByKey((x,y)=>(x._1+y._1,x._2+y._2))
      .map(x=>(x._1,x._2._1/x._2._2))

    val movies =sc.textFile("/home/bigdata3/ml-1m/movies.dat")
      .map(_.split("::"))
      .map(x => (x(0), x(1)))
      .join(avgRatings)
      .map(x=>(x._2._2,(x._1,x._2._1)))
      .sortByKey(false)
    movies.take(10).foreach(println)
    Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
DAG：
![图 21](images/7d80b14168ef21b549d46b8e02f4abb0ec3827f7fd5a36eb233e5c32199a7d5f.png)  
优化后的特征：每一个输入是一个程序分支，在本分支计算完成后与其他分支汇合。
程序运行结果：
![图 15](images/e545f854d0a54f28ae831cec01a67861184c0a461adf8546bed4d3d23d4df859.png)  
运行结果不合理，数据类型是Double，而评分都是5分，按常识，此电影所有人都评5分，只能说明评分数太少。需要对评分次数进行过滤。
#### 2.5 数据过滤
对评分次数进行过滤，只留评分次数>20的数据。
程序：
```java
package zufe.info.bigdata.xhm431.movieRating

import org.apache.spark.{SparkConf, SparkContext}

object movieRating {
  def main(args: Array[String]): Unit = {

    val conf=new SparkConf().setAppName("movieRating").setMaster("local[*]")
    val sc=new SparkContext(conf)
    val avgRatings=sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
      .map(_.split("::"))
      .map(x=>(x(1),(x(2).toDouble,1.toDouble)))
      .reduceByKey((x,y)=>(x._1+y._1,x._2+y._2))
      .filter(_._2._2>20)
      .map(x=>(x._1,x._2._1/x._2._2))

    val movies =sc.textFile("/home/bigdata3/ml-1m/movies.dat")
      .map(_.split("::"))
      .map(x => (x(0), x(1)))
      .join(avgRatings)
      .map(x=>(x._2._2,(x._1,x._2._1)))
      .sortByKey(false)
    movies.take(10).foreach(println)
    Thread.sleep(1000*60*5)
    sc.stop()
  }
}
```
DAG：
![图 23](images/1d0a9d1477cf430bc92ff50a9250db8e7c13f13089eccb46cf0a508ba228e5de.png)  

运行结果：
![图 16](images/d5c5e7b49c760158e7f9e5654ab2f47565ffc455a220fbedd2ed90131a61e48b.png)  

# 第五章 遇到的问题及解决办法
## 一、网络问题
### 1 虚拟机设置问题
管理器->设置->网络，检查选择的是否为桥接网卡，若不是，则改为桥接网卡。
![图 7](images/67d66305629e981ccbe805320a2563cabe3432ebac75775913cb76b229bb188f.png)  
### 2 ip地址被人占用
我们可以在桌面CTRL+R，打开cmd，通过ping+ip地址的方式检测当前使用的ip地址是否被占用。
![图 8](images/68644b97f118302357ed97d0ebb2fe656b4909b7802a88fd2a16d293a8b87371.png)  
若已被占有，我们可以通过Wire Connected->Wire Settings，在ipv4中修改ip地址。
![图 9](images/9aa2a79a659d8f7741942e59d8462898fb6dc6d5875d05529effd96b0e9f171c.png) 
## 二、环境配置问题
### 1 java、hadoop在全局中无法运行
输入vim .bashrc,发现是我们在填写环境变量时，多打了一个空格，导致该环境变量无效，取消空格即可。
![图 25](images/415fba35c9c992395b3d013f2965f897fb7a52502b47d2fb57e80b0fa362bcdd.png)  
### 2 配置文件后发现无法保存，提示没有权限
#### 2.1 强制保存退出
配置文件后点击ESC退出插入模式，直接使用w!强制保存退出（不推荐）
#### 2.2 添加管理员权限
进入文件前添加一个sudo，例如：sudo vim .bashrc
在输入当前密码后便能以管理员身份进入文本编辑中。
### 3 hadoop下的文件配置未更新
![图 26](images/23fb0a5a40bb2ab112d1b01058eb060629553bee74246f92750d26abe95a0289.png)  
hadoop下的文件未更新，而hadoopconfig下的文件配置并未出错。这说明是我们分配时出现了问题，我们选择重新进行分配。
输入以下代码，分配两个文件一个快捷功能。
```shell
chmod +x distributeLocal.sh distributeAll.sh
```
将文件配置分配到本地电脑中
```shell
bash distributeLocal.sh
```
将文件配置分配到集群各个节点上
```shell
bash distributeAll.sh
```

## 三、hadoop集群问题
### 1 第二次格式化前忘记删除所有节点的数据文件夹，导致进入Uber(安全)模式
在安全模式下输入指令,即可退出安全模式。
```shell
hadoop dfsadmin -safemode leave
```
![图 24](images/854aa2635b4810d1eb25d68f38b89c3a6205ab5aa322bc8095148733bc747dfa.png) 
### 2 连接到其他节点仍需输入密码
大概率是免密设置并未成功，以防万一，先删除已有免密后重新进行免密设置，传递公钥到所有节点。最后登录其他节点，若无需输入密码便能进入
```shell
rm -rf .ssh/
```
### 3 Hadoop启动后没有Name Node或DataNode进程
将所有节点的tmp目录和logs目录下的文件清空清空。
```shell
rm -rf *
```
重新格式化namenode
```shell
hadoop namenode -format
```
最后重新启动hadoop集群即可

# 第六章 总结
## 一、Linux
### 1 坚持使用命令行界面
与windows系统不同的是，Linux更常使用的是命令行界面，而不是图形化界面。从初学开始，我逐渐改掉了在图形化界面进行操作的习惯，在Linux系统中坚持使用命令行来完成有关功能。
### 2 常用命令
由于Linux更常使用命令行界面的原因，我们需要牢记大量常会用到的命令，这些简单的命令可以使我们平时的学习事半功倍。
## 二、Hadoop
### 1 集群搭建
集群搭建是一个复杂的过程，需要将多个节点连接到一起。在这个过程中一不小心就会出现大量问题。在集群搭建的过程中我们深入了解了hadoop完全分布式的强大功能。
## 三、Spark
### 1 计算速度快
Spark 基于内存计算，能够比基于磁盘的计算快很多。对于迭代式算法和交互式数据挖掘任务，这种速度优势尤为明显。
### 2 容错性高
Spark 提供了弹性分布式数据集（RDD）抽象，可以帮助开发人员更快地构建容错应用程序。
### 3 容易上手
Spark 支持多种语言，包括 Java、Scala、Python 和 R。它提供了丰富的内置 API，可以帮助开发人员更快地构建和运行应用程序。
## 四、实际应用：电影评分案例： 
最令我印象深刻的是利用Spark完成的电影评分案例。通过数据处理和分析，我们成功地找出了评分最高的10部电影。这个实例让我意识到了大数据技术在实际应用中的价值和强大功能。
## 五、课程总结
大数据开发技术是一门很有意思的课，我们第一次接触到了虚拟机和Linux系统，但这门课也有许多我们需要注意的事项。
### 1 上课专注
Linux系统对我们来说是较为陌生的系统，因此上课过程中如果我们一不留心就会错过重要步骤，从而一直跟不上。只有上课跟着老师的讲解一步一步来才不会出错。
### 2 课后花时间
hadoop集群搭建、scala环境配置、spark语言应用对我们来说基本都是第一次接触。由于电脑系统原因、代码输入原因，每个人都会出现不一样的错误。这些错误老师并不能完全解答，这就需要我们课后通过各自渠道更正错误。
### 3 命名问题
用户名、系统名等方面最好跟老师一样，因为我们是初学者，我们是跟着老师学习的。如果各自命名跟老师不一样，后续可能会出现老师成功了，但我们因为命名问题无法照搬，自己又无法解决的情况。
